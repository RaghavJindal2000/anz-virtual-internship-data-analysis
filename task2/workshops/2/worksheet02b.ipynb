{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP90051 Workshop 2\n",
    "## Part B: Bayesian inference\n",
    "\n",
    "***\n",
    "\n",
    "In this part of the workshop, we'll develop some intuition for priors and posteriors, which are crucial to Bayesian inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import bernoulli, beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. A lucky find\n",
    "\n",
    "On the way to class, you discover an unusual coin on the ground.\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/6/68/1_2_penny_Middlesex_DukeYork_1795_1ar85_%288737903267%29.jpg\" alt=\"Coin\" width=\"350\"/>\n",
    "\n",
    "As a dedicated student in statistical ML, you're interested in determining whether the coin is biased. \n",
    "More specifically, you want to estimate the probability $\\theta$ that the coin will land heads-up when you toss it.\n",
    "\n",
    "You can use the function below to simulate a coin toss: it returns `1` for heads and `0` for tails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toss_coin():\n",
    "    if bernoulli.rvs(p = (int.from_bytes(\"coin\".encode(), 'little') % 10000)/10000):\n",
    "        return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Prior belief\n",
    "Before you even toss the coin, you notice that the heads side appears to have more mass. \n",
    "Thus, your _prior belief_ is that $\\theta$ is slightly biased away from 0.5 towards 0â€”i.e. you expect tails are more likely.\n",
    "\n",
    "To quantify this prior belief, we assume that the prior distribution for $\\theta$ is $\\mathrm{Beta}(a,b)$, for some choice of the hyperparameters $a, b > 0$. \n",
    "(See [link](https://en.wikipedia.org/wiki/Beta_distribution) for info about the Beta distribution.)\n",
    "The prior probability density function for $\\theta$ is therefore given by:\n",
    "\n",
    "$$ p(\\theta) = \\frac{1}{B(a,b)} \\theta^{a-1} (1 - \\theta)^{b-1} $$\n",
    "where $B(a,b)$ is a special function called the _Beta function_.\n",
    "\n",
    "Select appropriate values for $a$ and $b$ by looking at the plot of $p(\\theta)$ below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "a = 0.5\n",
    "b=1\n",
    "theta = np.linspace(0, 1, 1000)\n",
    "plt.plot(theta, beta.pdf(theta, a, b), 'b-', lw=1)\n",
    "plt.xlabel(r'$\\theta$')\n",
    "plt.ylabel(r'$p(\\theta)$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Posterior updates\n",
    "Now toss the coin once and denote the outcome by $x_1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "x1 = toss_coin()\n",
    "print(x1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can update our belief about $\\theta$, based on this new evidence $x_1$.\n",
    "To do this we apply Bayes' rule to compute the posterior for $\\theta$:\n",
    "$$ p(\\theta | x_1) = \\frac{p(x_1 | \\theta) \\, p(\\theta)}{p(x_1)} \\propto p(x_1 | \\theta) \\, p(\\theta)$$\n",
    "where $p(\\theta)$ is the prior given above and \n",
    "$$ p(x_1 | \\theta) = \\theta^{x_1} (1 - \\theta)^{1 - x_1} $$\n",
    "is the likelihood.\n",
    "\n",
    "***\n",
    "**Exercise:** Show (on paper) that\n",
    "$$ p(\\theta | x_1) \\propto \\theta^{x_1 + a - 1} (1 - \\theta)^{(1 - x_1) + b - 1} $$\n",
    "which implies that $\\theta | x_1 \\sim \\mathrm{Beta}[x_1 + a, (1 - x_1) + b]$.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Toss the coin a second time, denoting the outcome by $x_2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "x2 = toss_coin()\n",
    "print(x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we want to update our belief about $\\theta$ based on the new information $x_2$. \n",
    "We take the previous posterior $p(\\theta|x_1)$ as the new prior and apply Bayes' rule:\n",
    "$$ p(\\theta | x_1, x_2) \\propto p(x_2 | \\theta) p(\\theta | x_1)$$\n",
    "\\[Note: We assume the tosses are independent, otherwise the likelihood for $x_2$ would depend on $x_1$.\\] \n",
    "This gives $\\theta | x_1, x_2 \\sim \\mathrm{Beta}[x_1 + x_2 + a, (2 - x_1 - x_2) + b]$.\n",
    "\n",
    "***\n",
    "**Exercise:** Show that for $n$ coin tosses, the posterior is $\\theta | x_1, \\ldots, x_n \\sim \\operatorname{Beta}[n_H + a, n - n_H + b]$ where $n_H = \\sum_{i = 1}^{n} x_i$ is the number of heads observed.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. MAP estimator and MLE estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The posterior $\\theta|x_1, \\ldots, x_n$ contains all the information we know about $\\theta$ after observing $n$ coin tosses.\n",
    "One way of obtaining a point estimate of $\\theta$ from the posterior, is to take the value with the maximum a posteriori probability (MAP):\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\hat{\\theta}_{MAP} &= \\arg \\max_{\\theta} p(\\theta|x_1, \\ldots, x_n) \\\\\n",
    "        & = \\frac{n_H + a - 1}{n + a + b - 2}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "In general, the MAP estimator gives a different result to the maximum likelihood estimator (MLE) for $\\theta$:\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\hat{\\theta}_{MLE} &=\\arg \\max_{\\theta} p(x_1, \\ldots, x_n|\\theta) \\\\\n",
    "        & = \\frac{n_H}{n}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "***\n",
    "**Exercise:** Derive the above results for $\\hat{\\theta}_{MAP}$ and  $\\hat{\\theta}_{MLE}$.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Convergence of the estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now toss the coin an additional 48 times (so that $n = 50$), recording $\\hat{\\theta}_{MLE}$ and $\\hat{\\theta}_{MAP}$ after each toss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "float() argument must be a string or a number, not 'ellipsis'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-29207036f1ea>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mnum_heads\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mtoss_coin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mtheta_map\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m...\u001b[0m \u001b[1;31m# fill in\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[0mtheta_mle\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m...\u001b[0m \u001b[1;31m# fill in\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: float() argument must be a string or a number, not 'ellipsis'"
     ]
    }
   ],
   "source": [
    "extra_tosses = 48\n",
    "num_tosses = 2 + extra_tosses\n",
    "num_heads = 0\n",
    "theta_map = np.zeros(num_tosses)\n",
    "theta_mle = np.zeros(num_tosses)\n",
    "for i in range(0, num_tosses):\n",
    "    if i == 0: \n",
    "        num_heads += x1 \n",
    "    elif i == 1:\n",
    "        num_heads += x2\n",
    "    else:\n",
    "        num_heads += toss_coin()\n",
    "    theta_map[i] = ... # fill in\n",
    "    theta_mle[i] = ... # fill in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the results below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(theta_map, label = \"MAP\")\n",
    "plt.plot(theta_mle, label = \"MLE\")\n",
    "plt.xlabel('Number of draws')\n",
    "plt.ylabel(r'$\\hat{\\theta}$')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions:** \n",
    "\n",
    "1. Is the coin biased?\n",
    "1. Do the MAP and MLE estimates converge to the same value for $\\theta$?\n",
    "1. What happens if you set $a = 1; b = 1$?\n",
    "1. How does the posterior distribution for $\\theta$ compare to the prior plotted above? (Use the code block below to plot the posterior.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(theta, beta.pdf(theta, a + num_heads, b + num_tosses - num_heads), 'b-', lw=1)\n",
    "plt.xlabel(r'$\\theta$')\n",
    "plt.ylabel(r'$p(\\theta|x_1, \\ldots, x_n)$')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
